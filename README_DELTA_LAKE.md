# Delta Lake - GovBR

Este projeto configura Delta Lake para ler os dados do MinIO como um banco relacional.

## üöÄ In√≠cio R√°pido

### Op√ß√£o 1: Usar Docker Compose (Recomendado)

```bash
# 1. Criar diret√≥rios necess√°rios
mkdir -p delta_scripts notebooks

# 2. Subir containers
docker-compose up -d

# 3. Executar setup inicial (converter Parquet para Delta)
docker exec -it govbr-delta-lake python /opt/spark/work-dir/delta_setup.py

# 4. Acessar Jupyter Lab
# Abra: http://localhost:8889
```

### Op√ß√£o 2: Executar Localmente

```bash
# 1. Instalar depend√™ncias
pip install delta-spark pyspark s3fs boto3

# 2. Executar setup
python delta_scripts/delta_setup.py

# 3. Executar consultas
python delta_scripts/query_delta.py
```

## üìä Estrutura de Tabelas

### Camada Bronze
- `bronze_municipios` - Munic√≠pios do IBGE
- `bronze_estados` - Estados do IBGE
- `bronze_bpc` - Dados BPC brutos
- `bronze_orgaos` - √ìrg√£os SIAFI

### Camada Prata
- `prata_dim_municipios` - Dimens√£o de munic√≠pios tratada
- `prata_dim_estados` - Dimens√£o de estados tratada
- `prata_fato_bpc` - Fato BPC tratado
- `prata_dim_orgaos` - Dimens√£o de √≥rg√£os

### Camada Ouro
- `ouro_dim_municipios` - Munic√≠pios enriquecidos
- `ouro_dim_estados` - Estados enriquecidos
- `ouro_fato_bpc` - Fato BPC enriquecido
- `ouro_agregacao_regiao` - Agrega√ß√£o por regi√£o
- `ouro_agregacao_estado` - Agrega√ß√£o por estado
- `ouro_top_municipios` - Top 10 munic√≠pios

## üîç Exemplos de Consultas SQL

### Consulta Simples
```sql
SELECT * FROM ouro_dim_estados LIMIT 10;
```

### Join entre Tabelas
```sql
SELECT 
    e.uf_sigla,
    e.uf_nome,
    SUM(f.valor) as total_bpc
FROM ouro_fato_bpc f
JOIN ouro_dim_estados e ON f.uf_sigla = e.uf_sigla
GROUP BY e.uf_sigla, e.uf_nome
ORDER BY total_bpc DESC;
```

### Agrega√ß√µes
```sql
SELECT 
    regiao_nome,
    COUNT(*) as total_municipios,
    AVG(populacao) as media_populacao
FROM ouro_dim_municipios
GROUP BY regiao_nome;
```

## üêç Usando Python/Spark

```python
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_packages

# Configurar Spark
builder = SparkSession.builder \
    .appName("GovBR Delta") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_packages(builder).getOrCreate()

# Ler tabela Delta
df = spark.read.format("delta").table("ouro_dim_estados")
df.show()

# Consulta SQL
spark.sql("SELECT * FROM ouro_dim_estados LIMIT 10").show()
```

## üîß Configura√ß√£o

### Vari√°veis de Ambiente
- `AWS_ACCESS_KEY_ID`: Chave de acesso MinIO (admin)
- `AWS_SECRET_ACCESS_KEY`: Chave secreta MinIO
- `AWS_ENDPOINT_URL`: URL do MinIO
- `MINIO_BUCKET`: Nome do bucket (govbr)

### Portas
- `8080`: Spark UI
- `4040`: Spark Application UI
- `8889`: Jupyter Lab

## üìù Notas

- Delta Lake fornece transa√ß√µes ACID sobre dados Parquet
- Suporta versionamento e time travel
- Compat√≠vel com SQL padr√£o
- Integra√ß√£o nativa com Spark

## üÜò Troubleshooting

### Erro de conex√£o com MinIO
Verifique se as credenciais est√£o corretas e o endpoint est√° acess√≠vel.

### Tabelas n√£o encontradas
Execute `delta_setup.py` para converter Parquet em Delta Lake.

### Problemas com SSL
Se houver problemas com SSL, ajuste `spark.hadoop.fs.s3a.connection.ssl.enabled` para `false`.
