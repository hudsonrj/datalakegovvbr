{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Configura√ß√£o do Spark - SOLU√á√ÉO PARA CONNECTION REFUSED\n",
    "\n",
    "**Execute este notebook primeiro antes de usar Spark em outros notebooks!**\n",
    "\n",
    "Este notebook resolve os problemas de:\n",
    "- ‚ùå `[Errno 111] Connection refused`\n",
    "- ‚ùå `Py4JNetworkError: Answer from Java side is empty`\n",
    "\n",
    "**Melhorias inclu√≠das:**\n",
    "- ‚úÖ Verifica√ß√£o e libera√ß√£o autom√°tica de portas\n",
    "- ‚úÖ Portas din√¢micas para evitar conflitos\n",
    "- ‚úÖ Configura√ß√µes robustas de rede e timeout\n",
    "- ‚úÖ Detec√ß√£o autom√°tica de problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîß CONFIGURA√á√ÉO SPARK - CORRE√á√ÉO CONNECTION REFUSED\n",
      "================================================================================\n",
      "‚ÑπÔ∏è  Parando SparkContext global...\n",
      "‚úÖ Sess√µes Spark paradas\n",
      "\n",
      "‚ÑπÔ∏è  Matando processos Java do Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ambiente completamente limpo\n"
     ]
    }
   ],
   "source": [
    "# PASSO 1: Limpar ambiente e processos Spark COMPLETAMENTE\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß CONFIGURA√á√ÉO SPARK - CORRE√á√ÉO CONNECTION REFUSED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Limpar TODAS as sess√µes Spark existentes de forma agressiva\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark import SparkContext\n",
    "    \n",
    "    # Tentar parar todas as sess√µes ativas\n",
    "    try:\n",
    "        active_session = SparkSession.getActiveSession()\n",
    "        if active_session:\n",
    "            print(\"‚ÑπÔ∏è  Parando Spark Session ativa...\")\n",
    "            try:\n",
    "                active_session.sparkContext.stop()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                active_session.stop()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Tentar parar SparkContext global se existir\n",
    "    try:\n",
    "        sc = SparkContext._active_spark_context\n",
    "        if sc:\n",
    "            print(\"‚ÑπÔ∏è  Parando SparkContext global...\")\n",
    "            sc.stop()\n",
    "            time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Limpar refer√™ncias globais do PySpark\n",
    "    try:\n",
    "        SparkSession._instantiatedContext = None\n",
    "        SparkContext._active_spark_context = None\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"‚úÖ Sess√µes Spark paradas\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  PySpark n√£o importado ainda\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro ao limpar sess√µes: {e}\")\n",
    "\n",
    "# Matar TODOS os processos Java relacionados ao Spark\n",
    "print(\"\\n‚ÑπÔ∏è  Matando processos Java do Spark...\")\n",
    "for pattern in [\"org.apache.spark\", \"java.*spark\", \"SparkSubmit\"]:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"pgrep\", \"-f\", pattern],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=5\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            for pid in pids:\n",
    "                if pid and pid.isdigit():\n",
    "                    try:\n",
    "                        print(f\"  ‚ÑπÔ∏è  Matando processo PID: {pid}\")\n",
    "                        subprocess.run([\"kill\", \"-9\", pid], check=False, timeout=2)\n",
    "                    except:\n",
    "                        pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "time.sleep(3)  # Aguardar processos terminarem completamente\n",
    "\n",
    "# Limpar vari√°veis globais Python que podem manter refer√™ncias\n",
    "try:\n",
    "    import __main__\n",
    "    if hasattr(__main__, 'spark'):\n",
    "        delattr(__main__, 'spark')\n",
    "    if hasattr(__main__, 'sc'):\n",
    "        delattr(__main__, 'sc')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# For√ßar garbage collection m√∫ltiplas vezes\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"‚úÖ Ambiente completamente limpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nenhuma porta em conflito encontrada\n"
     ]
    }
   ],
   "source": [
    "# PASSO 2: Verificar e liberar portas em uso\n",
    "import socket\n",
    "\n",
    "def check_port(port):\n",
    "    \"\"\"Verifica se uma porta est√° em uso\"\"\"\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)\n",
    "        result = sock.connect_ex(('127.0.0.1', port))\n",
    "        sock.close()\n",
    "        return result == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def kill_process_on_port(port):\n",
    "    \"\"\"Mata processo usando uma porta espec√≠fica\"\"\"\n",
    "    try:\n",
    "        # Tentar lsof\n",
    "        result = subprocess.run(\n",
    "            [\"lsof\", \"-ti\", f\":{port}\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=2\n",
    "        )\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            for pid in pids:\n",
    "                if pid and pid.isdigit():\n",
    "                    try:\n",
    "                        subprocess.run([\"kill\", \"-9\", pid], check=False, timeout=2)\n",
    "                    except:\n",
    "                        pass\n",
    "            time.sleep(1)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Verificar e liberar portas comuns do Spark\n",
    "spark_ports = [4040, 4041, 4042, 8080, 8081, 7077, 7078]\n",
    "portas_liberadas = []\n",
    "for port in spark_ports:\n",
    "    if check_port(port):\n",
    "        print(f\"‚ö†Ô∏è  Porta {port} est√° em uso\")\n",
    "        if kill_process_on_port(port):\n",
    "            if not check_port(port):\n",
    "                print(f\"‚úÖ Porta {port} liberada\")\n",
    "                portas_liberadas.append(port)\n",
    "\n",
    "if portas_liberadas:\n",
    "    print(f\"‚úÖ {len(portas_liberadas)} portas liberadas\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhuma porta em conflito encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JAVA_HOME configurado: /usr/lib/jvm/java-17-openjdk-amd64\n",
      "‚úÖ Vari√°veis de ambiente configuradas\n"
     ]
    }
   ],
   "source": [
    "# PASSO 3: Configurar vari√°veis de ambiente\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "# Verificar JAVA_HOME\n",
    "java_home_candidates = [\n",
    "    '/usr/lib/jvm/java-17-openjdk-amd64',\n",
    "    '/usr/lib/jvm/java-11-openjdk-amd64',\n",
    "    '/usr/lib/jvm/java-8-openjdk-amd64',\n",
    "    os.environ.get('JAVA_HOME', ''),\n",
    "]\n",
    "\n",
    "java_home = None\n",
    "for candidate in java_home_candidates:\n",
    "    if candidate and os.path.exists(candidate):\n",
    "        java_home = candidate\n",
    "        break\n",
    "\n",
    "if java_home:\n",
    "    os.environ['JAVA_HOME'] = java_home\n",
    "    print(f\"‚úÖ JAVA_HOME configurado: {java_home}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  JAVA_HOME n√£o encontrado, tentando detectar...\")\n",
    "    try:\n",
    "        result = subprocess.run([\"which\", \"java\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            java_path = result.stdout.strip()\n",
    "            java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            print(f\"‚úÖ JAVA_HOME detectado: {java_home}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Configura√ß√µes de rede\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "os.environ['SPARK_MASTER'] = 'local[*]'\n",
    "os.environ['SPARK_DRIVER_HOST'] = '127.0.0.1'\n",
    "os.environ['SPARK_DRIVER_BIND_ADDRESS'] = '127.0.0.1'\n",
    "os.environ['SPARK_LOCAL_DIRS'] = '/tmp/spark-local'\n",
    "\n",
    "os.makedirs('/tmp/spark-local', exist_ok=True)\n",
    "print(\"‚úÖ Vari√°veis de ambiente configuradas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PySpark importado\n",
      "‚ÑπÔ∏è  Vers√£o Spark: 3.5.0\n",
      "‚ÑπÔ∏è  Delta Lake ser√° carregado via JARs\n"
     ]
    }
   ],
   "source": [
    "# PASSO 4: Importar PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "print(\"‚úÖ PySpark importado\")\n",
    "print(f\"‚ÑπÔ∏è  Vers√£o Spark: {pyspark.__version__}\")\n",
    "print(\"‚ÑπÔ∏è  Delta Lake ser√° carregado via JARs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Usando Delta Lake 3.0.0 para Spark 3.5.x\n",
      "‚úÖ Configura√ß√µes definidas\n"
     ]
    }
   ],
   "source": [
    "# PASSO 5: Configura√ß√µes MinIO e vers√µes\n",
    "MINIO_ENDPOINT = \"ch8ai-minio.l6zv5a.easypanel.host\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"1q2w3e4r\"\n",
    "BUCKET_NAME = \"govbr\"\n",
    "\n",
    "# Determinar vers√£o do Delta Lake compat√≠vel\n",
    "spark_version = pyspark.__version__\n",
    "if spark_version.startswith(\"4.\"):\n",
    "    delta_package = \"io.delta:delta-spark_2.13:4.0.0\"\n",
    "    hadoop_package = \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "    aws_package = \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    print(f\"‚úÖ Usando Delta Lake 4.0.0 para Spark 4.x\")\n",
    "elif spark_version.startswith(\"3.5\"):\n",
    "    delta_package = \"io.delta:delta-spark_2.12:3.0.0\"\n",
    "    hadoop_package = \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "    aws_package = \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    print(f\"‚úÖ Usando Delta Lake 3.0.0 para Spark 3.5.x\")\n",
    "else:\n",
    "    delta_package = \"io.delta:delta-spark_2.12:2.4.0\"\n",
    "    hadoop_package = \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "    aws_package = \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    print(f\"‚úÖ Usando Delta Lake 2.4.0 (padr√£o)\")\n",
    "\n",
    "print(\"‚úÖ Configura√ß√µes definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  Verificando Java...\n",
      "‚úÖ Java est√° dispon√≠vel\n",
      "   openjdk version \"17.0.8.1\" 2023-08-24\n",
      "‚úÖ JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n",
      "   ‚úÖ JAVA_HOME v√°lido\n",
      "\n",
      "‚ÑπÔ∏è  Configurando Spark Builder...\n",
      "‚úÖ Builder configurado com:\n",
      "   - Portas din√¢micas (evita Connection refused)\n",
      "   - Timeouts aumentados\n",
      "   - Garbage Collector otimizado\n",
      "   - Suporte a S3A (MinIO)\n"
     ]
    }
   ],
   "source": [
    "# PASSO 6: Verificar Java antes de criar Builder\n",
    "print(\"‚ÑπÔ∏è  Verificando Java...\")\n",
    "try:\n",
    "    result = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Java est√° dispon√≠vel\")\n",
    "        print(f\"   {result.stderr.split(chr(10))[0] if result.stderr else 'Vers√£o detectada'}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Java pode n√£o estar configurado corretamente\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro ao verificar Java: {e}\")\n",
    "\n",
    "# Verificar JAVA_HOME\n",
    "java_home = os.environ.get('JAVA_HOME')\n",
    "if java_home:\n",
    "    print(f\"‚úÖ JAVA_HOME: {java_home}\")\n",
    "    if os.path.exists(java_home):\n",
    "        print(\"   ‚úÖ JAVA_HOME v√°lido\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  JAVA_HOME aponta para diret√≥rio inexistente\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  JAVA_HOME n√£o configurado\")\n",
    "\n",
    "# PASSO 6b: Criar Spark Builder com configura√ß√µes robustas\n",
    "# ‚≠ê IMPORTANTE: Portas din√¢micas (0) evitam \"Connection refused\"\n",
    "print(\"\\n‚ÑπÔ∏è  Configurando Spark Builder...\")\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"GovBR Data Lake\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"0\") \\\n",
    "    .config(\"spark.blockManager.port\", \"0\") \\\n",
    "    .config(\"spark.broadcast.port\", \"0\") \\\n",
    "    .config(\"spark.fileserver.port\", \"0\") \\\n",
    "    .config(\"spark.replClassServer.port\", \"0\") \\\n",
    "    .config(\"spark.ui.port\", \"0\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{delta_package},{hadoop_package},{aws_package}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"https://{MINIO_ENDPOINT}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"15\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \n",
    "            \"-Dio.netty.tryReflectionSetAccessible=true \" +\n",
    "            \"-XX:+UseG1GC \" +\n",
    "            \"-XX:MaxGCPauseMillis=200 \" +\n",
    "            \"-Djava.net.preferIPv4Stack=true \" +\n",
    "            \"-Djava.awt.headless=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \n",
    "            \"-Dio.netty.tryReflectionSetAccessible=true \" +\n",
    "            \"-XX:+UseG1GC \" +\n",
    "            \"-Djava.net.preferIPv4Stack=true \" +\n",
    "            \"-Djava.awt.headless=true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "print(\"‚úÖ Builder configurado com:\")\n",
    "print(\"   - Portas din√¢micas (evita Connection refused)\")\n",
    "print(\"   - Timeouts aumentados\")\n",
    "print(\"   - Garbage Collector otimizado\")\n",
    "print(\"   - Suporte a S3A (MinIO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  Verificando ambiente Java antes de criar Spark Session...\n",
      "‚úÖ Java est√° dispon√≠vel\n",
      "\n",
      "‚ÑπÔ∏è  Limpeza final antes de criar sess√£o...\n",
      "‚úÖ Ambiente limpo. Criando Spark Session...\n",
      "‚ÑπÔ∏è  Usando portas din√¢micas para evitar Connection refused\n",
      "‚ÑπÔ∏è  Criando Spark Session (JVM ser√° inicializado automaticamente)...\n",
      "‚ÑπÔ∏è  Aguardando Spark inicializar completamente...\n",
      "‚ÑπÔ∏è  Verificando conectividade...\n",
      "\n",
      "‚úÖ Spark Session criada com sucesso!\n",
      "   Vers√£o: 3.5.0\n",
      "   App Name: GovBR Data Lake\n",
      "   Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "# PASSO 7: Verificar e inicializar JVM antes de criar Spark Session\n",
    "print(\"‚ÑπÔ∏è  Verificando ambiente Java antes de criar Spark Session...\")\n",
    "\n",
    "# Verificar se Java est√° realmente dispon√≠vel\n",
    "java_ok = False\n",
    "try:\n",
    "    result = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0 or result.stderr:\n",
    "        java_ok = True\n",
    "        print(\"‚úÖ Java est√° dispon√≠vel\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Java pode n√£o estar funcionando corretamente\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao verificar Java: {e}\")\n",
    "    print(\"\\nüí° CR√çTICO: Java n√£o est√° dispon√≠vel!\")\n",
    "    print(\"   Execute: sudo apt-get update && sudo apt-get install -y openjdk-17-jdk\")\n",
    "    raise RuntimeError(\"Java n√£o est√° dispon√≠vel. Instale Java antes de continuar.\")\n",
    "\n",
    "# Verificar JAVA_HOME\n",
    "java_home = os.environ.get('JAVA_HOME')\n",
    "if not java_home or not os.path.exists(java_home):\n",
    "    print(\"‚ö†Ô∏è  JAVA_HOME n√£o configurado ou inv√°lido\")\n",
    "    # Tentar detectar automaticamente\n",
    "    try:\n",
    "        result = subprocess.run([\"which\", \"java\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            java_path = result.stdout.strip()\n",
    "            java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            print(f\"‚úÖ JAVA_HOME detectado: {java_home}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Limpar completamente qualquer refer√™ncia ao SparkContext antes de criar\n",
    "print(\"\\n‚ÑπÔ∏è  Limpeza final antes de criar sess√£o...\")\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    # For√ßar limpeza do SparkContext\n",
    "    if hasattr(SparkContext, '_active_spark_context'):\n",
    "        SparkContext._active_spark_context = None\n",
    "    if hasattr(SparkContext, '_jvm'):\n",
    "        SparkContext._jvm = None\n",
    "    if hasattr(SparkContext, '_gateway'):\n",
    "        SparkContext._gateway = None\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Limpar refer√™ncias de SparkSession\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    existing = SparkSession.getActiveSession()\n",
    "    if existing:\n",
    "        print(\"‚ö†Ô∏è  ATEN√á√ÉO: Ainda h√° uma sess√£o ativa! Parando...\")\n",
    "        try:\n",
    "            existing.stop()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            pass\n",
    "    # Limpar refer√™ncias globais\n",
    "    SparkSession._instantiatedContext = None\n",
    "    SparkSession._instantiatedSession = None\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# For√ßar garbage collection antes de criar\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"‚úÖ Ambiente limpo. Criando Spark Session...\")\n",
    "print(\"‚ÑπÔ∏è  Usando portas din√¢micas para evitar Connection refused\")\n",
    "\n",
    "try:\n",
    "    # ‚≠ê CR√çTICO: Usar builder diretamente, mas garantir que JVM seja inicializado\n",
    "    print(\"‚ÑπÔ∏è  Criando Spark Session (JVM ser√° inicializado automaticamente)...\")\n",
    "    \n",
    "    # O builder.getOrCreate() vai criar o SparkConf internamente\n",
    "    # e isso vai inicializar o JVM atrav√©s do Py4J\n",
    "    # Mas precisamos garantir que n√£o h√° conflitos\n",
    "    \n",
    "    # Criar sess√£o diretamente usando o builder\n",
    "    # O PySpark vai inicializar o JVM automaticamente quando necess√°rio\n",
    "    spark = builder.getOrCreate()\n",
    "    \n",
    "    # Aguardar Spark inicializar completamente\n",
    "    print(\"‚ÑπÔ∏è  Aguardando Spark inicializar completamente...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Verificar se Spark est√° respondendo\n",
    "    print(\"‚ÑπÔ∏è  Verificando conectividade...\")\n",
    "    version = spark.version\n",
    "    app_name = spark.sparkContext.appName\n",
    "    master = spark.sparkContext.master\n",
    "    \n",
    "    print(f\"\\n‚úÖ Spark Session criada com sucesso!\")\n",
    "    print(f\"   Vers√£o: {version}\")\n",
    "    print(f\"   App Name: {app_name}\")\n",
    "    print(f\"   Master: {master}\")\n",
    "    \n",
    "except ConnectionRefusedError as e:\n",
    "    print(f\"\\n‚ùå Connection Refused: O servidor Java do Spark n√£o est√° respondendo\")\n",
    "    print(f\"   Erro: {e}\")\n",
    "    print(\"\\nüí° DIAGN√ìSTICO:\")\n",
    "    print(\"1. Verifique processos Java:\")\n",
    "    print(\"   !ps aux | grep java\")\n",
    "    print(\"2. Verifique portas:\")\n",
    "    print(\"   !netstat -tuln | grep -E '4040|8080|7077'\")\n",
    "    print(\"3. Verifique JAVA_HOME:\")\n",
    "    print(f\"   !echo $JAVA_HOME\")\n",
    "    print(\"4. Reinicie o container:\")\n",
    "    print(\"   docker restart govbr-jupyter-delta\")\n",
    "    print(\"5. Execute o script completo:\")\n",
    "    print(\"   exec(open('/home/jovyan/work/fix_spark_py4j.py').read())\")\n",
    "    raise\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao criar Spark Session: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TENTANDO MODO FALLBACK (sem Delta, configura√ß√£o m√≠nima)...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Fallback: criar Spark Session com configura√ß√£o m√≠nima\n",
    "    try:\n",
    "        conf_fallback = SparkConf()\n",
    "        conf_fallback.set(\"spark.app.name\", \"GovBR Data Lake (Fallback)\")\n",
    "        conf_fallback.set(\"spark.master\", \"local[*]\")\n",
    "        conf_fallback.set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        conf_fallback.set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        conf_fallback.set(\"spark.driver.port\", \"0\")\n",
    "        conf_fallback.set(\"spark.blockManager.port\", \"0\")\n",
    "        conf_fallback.set(\"spark.ui.port\", \"0\")\n",
    "        conf_fallback.set(\"spark.driver.memory\", \"2g\")\n",
    "        conf_fallback.set(\"spark.executor.memory\", \"2g\")\n",
    "        conf_fallback.set(\"spark.network.timeout\", \"1200s\")\n",
    "        conf_fallback.set(\"spark.driver.extraJavaOptions\", \n",
    "                \"-Dio.netty.tryReflectionSetAccessible=true \" +\n",
    "                \"-Djava.net.preferIPv4Stack=true \" +\n",
    "                \"-Djava.awt.headless=true\")\n",
    "        \n",
    "        print(\"‚ÑπÔ∏è  Criando SparkConf fallback...\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        print(\"‚ÑπÔ∏è  Criando Spark Session fallback...\")\n",
    "        spark = SparkSession.builder.config(conf=conf_fallback).getOrCreate()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        version = spark.version\n",
    "        print(\"‚úÖ Spark Session criada em modo fallback (sem Delta)\")\n",
    "        print(f\"   Vers√£o: {version}\")\n",
    "        print(\"‚ö†Ô∏è  ATEN√á√ÉO: Delta Lake n√£o est√° dispon√≠vel. Use apenas Parquet.\")\n",
    "    except Exception as e3:\n",
    "        print(f\"‚ùå Erro tamb√©m no modo fallback: {e3}\")\n",
    "        print(\"\\nüí° SOLU√á√ïES CR√çTICAS:\")\n",
    "        print(\"1. Reinicie o container: docker restart govbr-jupyter-delta\")\n",
    "        print(\"2. Verifique se Java est√° instalado: java -version\")\n",
    "        print(\"3. Verifique JAVA_HOME: echo $JAVA_HOME\")\n",
    "        print(\"4. Execute o script completo: exec(open('/home/jovyan/work/fix_spark_py4j.py').read())\")\n",
    "        print(\"5. Verifique logs: docker logs govbr-jupyter-delta\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTE DE CONECTIVIDADE\n",
      "================================================================================\n",
      "‚úÖ Teste Py4J bem-sucedido! (5 registros)\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PASSO 8: Teste de conectividade Py4J\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTE DE CONECTIVIDADE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    test_df = spark.range(5)\n",
    "    count = test_df.count()\n",
    "    if count == 5:\n",
    "        print(f\"‚úÖ Teste Py4J bem-sucedido! ({count} registros)\")\n",
    "        test_df.show()\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Teste retornou resultado inesperado: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no teste Py4J: {e}\")\n",
    "    print(\"üí° Execute o notebook FIX_PY4J_ERROR.ipynb para corre√ß√£o completa\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ SPARK SESSION PRONTA PARA USO!\n",
      "================================================================================\n",
      "\n",
      "A vari√°vel 'spark' est√° dispon√≠vel para uso nos notebooks.\n",
      "Use spark para criar DataFrames e executar consultas SQL.\n"
     ]
    }
   ],
   "source": [
    "# Tornar spark dispon√≠vel globalmente\n",
    "import __main__\n",
    "__main__.spark = spark\n",
    "globals()['spark'] = spark\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ SPARK SESSION PRONTA PARA USO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nA vari√°vel 'spark' est√° dispon√≠vel para uso nos notebooks.\")\n",
    "print(\"Use spark para criar DataFrames e executar consultas SQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Pronto!\n",
    "\n",
    "Agora voc√™ pode usar o Spark normalmente em outros notebooks. A vari√°vel `spark` est√° dispon√≠vel.\n",
    "\n",
    "### Exemplo de uso:\n",
    "\n",
    "```python\n",
    "# Ler dados do MinIO\n",
    "df = spark.read.parquet(\"s3a://govbr/bronze/ibge/municipios/\")\n",
    "df.show()\n",
    "\n",
    "# Executar consultas SQL\n",
    "df.createOrReplaceTempView(\"municipios\")\n",
    "result = spark.sql(\"SELECT * FROM municipios LIMIT 10\")\n",
    "result.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+--------+---------+-----------+---------------+-----------------+--------------+-----------------+--------+\n",
      "|codigo_ibge|           municipio|uf_sigla| uf_nome|regiao_id|regiao_nome|microrregiao_id|microrregiao_nome|mesorregiao_id| mesorregiao_nome|      dt|\n",
      "+-----------+--------------------+--------+--------+---------+-----------+---------------+-----------------+--------------+-----------------+--------+\n",
      "|    1100015|Alta Floresta D'O...|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100023|           Ariquemes|      RO|Rond√¥nia|      1.0|      Norte|        11003.0|        Ariquemes|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100031|              Cabixi|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100049|              Cacoal|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100056|          Cerejeiras|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100064|   Colorado do Oeste|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100072|          Corumbiara|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100080|       Costa Marques|      RO|Rond√¥nia|      1.0|      Norte|        11002.0|    Guajar√°-Mirim|        1101.0|  Madeira-Guapor√©|20251114|\n",
      "|    1100098|     Espig√£o D'Oeste|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100106|       Guajar√°-Mirim|      RO|Rond√¥nia|      1.0|      Norte|        11002.0|    Guajar√°-Mirim|        1101.0|  Madeira-Guapor√©|20251114|\n",
      "|    1100114|                Jaru|      RO|Rond√¥nia|      1.0|      Norte|        11004.0|        Ji-Paran√°|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100122|           Ji-Paran√°|      RO|Rond√¥nia|      1.0|      Norte|        11004.0|        Ji-Paran√°|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100130|  Machadinho D'Oeste|      RO|Rond√¥nia|      1.0|      Norte|        11003.0|        Ariquemes|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100148|Nova Brasil√¢ndia ...|      RO|Rond√¥nia|      1.0|      Norte|        11005.0| Alvorada D'Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100155| Ouro Preto do Oeste|      RO|Rond√¥nia|      1.0|      Norte|        11004.0|        Ji-Paran√°|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100189|       Pimenta Bueno|      RO|Rond√¥nia|      1.0|      Norte|        11007.0|          Vilhena|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100205|         Porto Velho|      RO|Rond√¥nia|      1.0|      Norte|        11001.0|      Porto Velho|        1101.0|  Madeira-Guapor√©|20251114|\n",
      "|    1100254|   Presidente M√©dici|      RO|Rond√¥nia|      1.0|      Norte|        11004.0|        Ji-Paran√°|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100262|          Rio Crespo|      RO|Rond√¥nia|      1.0|      Norte|        11003.0|        Ariquemes|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100288|      Rolim de Moura|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "+-----------+--------------------+--------+--------+---------+-----------+---------------+-----------------+--------------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------------------+--------+--------+---------+-----------+---------------+-----------------+--------------+-----------------+--------+\n",
      "|codigo_ibge|           municipio|uf_sigla| uf_nome|regiao_id|regiao_nome|microrregiao_id|microrregiao_nome|mesorregiao_id| mesorregiao_nome|      dt|\n",
      "+-----------+--------------------+--------+--------+---------+-----------+---------------+-----------------+--------------+-----------------+--------+\n",
      "|    1100015|Alta Floresta D'O...|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100023|           Ariquemes|      RO|Rond√¥nia|      1.0|      Norte|        11003.0|        Ariquemes|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100031|              Cabixi|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100049|              Cacoal|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100056|          Cerejeiras|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100064|   Colorado do Oeste|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100072|          Corumbiara|      RO|Rond√¥nia|      1.0|      Norte|        11008.0|Colorado do Oeste|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100080|       Costa Marques|      RO|Rond√¥nia|      1.0|      Norte|        11002.0|    Guajar√°-Mirim|        1101.0|  Madeira-Guapor√©|20251114|\n",
      "|    1100098|     Espig√£o D'Oeste|      RO|Rond√¥nia|      1.0|      Norte|        11006.0|           Cacoal|        1102.0|Leste Rondoniense|20251114|\n",
      "|    1100106|       Guajar√°-Mirim|      RO|Rond√¥nia|      1.0|      Norte|        11002.0|    Guajar√°-Mirim|        1101.0|  Madeira-Guapor√©|20251114|\n",
      "+-----------+--------------------+--------+--------+---------+-----------+---------------+-----------------+--------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ler dados do MinIO\n",
    "df = spark.read.parquet(\"s3a://govbr/bronze/ibge/municipios/\")\n",
    "df.show()\n",
    "\n",
    "# Executar consultas SQL\n",
    "df.createOrReplaceTempView(\"municipios\")\n",
    "result = spark.sql(\"SELECT * FROM municipios LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
