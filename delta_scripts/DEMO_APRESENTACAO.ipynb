{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ GovBR Data Lake - Demonstra√ß√£o Completa\n",
    "\n",
    "## Apresenta√ß√£o da Solu√ß√£o de Data Lake com Arquitetura Medallion\n",
    "\n",
    "Este notebook demonstra a arquitetura completa do Data Lake GovBR:\n",
    "- **Bronze**: Dados brutos das APIs\n",
    "- **Prata**: Dados transformados e relacionados\n",
    "- **Ouro**: Dados enriquecidos para an√°lise\n",
    "\n",
    "### Tecnologias Utilizadas\n",
    "- ‚úÖ Apache Spark 4.0.1\n",
    "- ‚úÖ Delta Lake (via JARs)\n",
    "- ‚úÖ MinIO (S3-compatible)\n",
    "- ‚úÖ Python/PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark funcional\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è IMPORTANTE: Execute primeiro o notebook CONFIGURAR_SPARK.ipynb\n",
    "# Se ainda n√£o executou, execute o script de corre√ß√£o:\n",
    "import sys\n",
    "\n",
    "def verificar_spark_funcional():\n",
    "    \"\"\"Verifica se Spark est√° funcional, n√£o apenas se existe\"\"\"\n",
    "    try:\n",
    "        spark\n",
    "    except NameError:\n",
    "        return False, \"Spark n√£o existe\"\n",
    "    \n",
    "    try:\n",
    "        # Tentar acessar uma propriedade simples\n",
    "        _ = spark.sparkContext\n",
    "        # Tentar criar um DataFrame de teste\n",
    "        test_df = spark.range(1)\n",
    "        test_df.collect()\n",
    "        return True, \"Spark funcional\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Spark existe mas n√£o est√° funcional: {e}\"\n",
    "\n",
    "spark_ok, mensagem = verificar_spark_funcional()\n",
    "\n",
    "if spark_ok:\n",
    "    print(f\"‚úÖ {mensagem}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {mensagem}\")\n",
    "    \n",
    "    # Verificar se √© erro de Connection Refused (sess√£o zombie)\n",
    "    if \"Connection refused\" in str(mensagem) or \"Errno 111\" in str(mensagem):\n",
    "        print(\"\\nüîß Detectado: Sess√£o Spark 'zombie' (Connection Refused)\")\n",
    "        print(\"   Recuperando Spark Session...\")\n",
    "        try:\n",
    "            exec(open('/home/jovyan/work/recuperar_spark.py').read())\n",
    "            # Verificar novamente\n",
    "            spark_ok, mensagem = verificar_spark_funcional()\n",
    "            if spark_ok:\n",
    "                print(\"‚úÖ Spark Session recuperada com sucesso!\")\n",
    "            else:\n",
    "                print(f\"‚ùå Falha na recupera√ß√£o: {mensagem}\")\n",
    "                print(\"üí° Execute CONFIGURAR_SPARK.ipynb manualmente\")\n",
    "        except (Exception, SystemExit, RuntimeError) as e:\n",
    "            print(f\"‚ùå Erro ao recuperar: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"üí° Execute CONFIGURAR_SPARK.ipynb manualmente\")\n",
    "    else:\n",
    "        print(\"\\nüîß Tentando configurar Spark automaticamente...\")\n",
    "        try:\n",
    "            # Tentar vers√£o V2 primeiro (mais robusta)\n",
    "            try:\n",
    "                exec(open('/home/jovyan/work/fix_spark_py4j_v2.py').read())\n",
    "            except:\n",
    "                # Fallback para vers√£o original\n",
    "                exec(open('/home/jovyan/work/fix_spark_py4j.py').read())\n",
    "            # Verificar novamente\n",
    "            spark_ok, mensagem = verificar_spark_funcional()\n",
    "            if spark_ok:\n",
    "                print(\"‚úÖ Spark configurado com sucesso!\")\n",
    "            else:\n",
    "                print(f\"‚ùå Falha ao configurar: {mensagem}\")\n",
    "                print(\"üí° Execute CONFIGURAR_SPARK.ipynb manualmente\")\n",
    "        except (Exception, SystemExit, RuntimeError) as e:\n",
    "            print(f\"‚ùå Erro ao executar script de corre√ß√£o: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"üí° Execute manualmente: CONFIGURAR_SPARK.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Vis√£o Geral da Arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GOVBR DATA LAKE - ARQUITETURA MEDALLION\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Spark vers√£o: 3.5.0\n",
      "‚úÖ App: GovBR Data Lake - JARs Manuais\n",
      "‚úÖ Master: local[1]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GOVBR DATA LAKE - ARQUITETURA MEDALLION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fun√ß√£o auxiliar para verificar Spark de forma segura\n",
    "def verificar_spark_info():\n",
    "    \"\"\"Verifica informa√ß√µes do Spark com tratamento robusto de erros\"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    # Tentar obter vers√£o\n",
    "    try:\n",
    "        info['version'] = spark.version\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            # M√©todo alternativo via SparkContext\n",
    "            info['version'] = spark.sparkContext.version\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel obter vers√£o: {e2}\")\n",
    "            info['version'] = \"N/A (erro Py4J)\"\n",
    "    \n",
    "    # Tentar obter app name\n",
    "    try:\n",
    "        info['app_name'] = spark.sparkContext.appName\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel obter appName: {e}\")\n",
    "        info['app_name'] = \"N/A\"\n",
    "    \n",
    "    # Tentar obter master\n",
    "    try:\n",
    "        info['master'] = spark.sparkContext.master\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel obter master: {e}\")\n",
    "        info['master'] = \"N/A\"\n",
    "    \n",
    "    # Teste b√°sico de funcionalidade\n",
    "    try:\n",
    "        test_df = spark.range(1)\n",
    "        test_df.collect()\n",
    "        info['funcional'] = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Spark n√£o est√° totalmente funcional: {e}\")\n",
    "        info['funcional'] = False\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Verificar Spark com tratamento de erros robusto\n",
    "try:\n",
    "    spark_info = verificar_spark_info()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Spark vers√£o: {spark_info.get('version', 'N/A')}\")\n",
    "    print(f\"‚úÖ App: {spark_info.get('app_name', 'N/A')}\")\n",
    "    print(f\"‚úÖ Master: {spark_info.get('master', 'N/A')}\")\n",
    "    \n",
    "    if not spark_info.get('funcional', False):\n",
    "        print(\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Spark Session existe mas pode n√£o estar totalmente funcional\")\n",
    "        print(\"üí° Se houver erros, execute CONFIGURAR_SPARK.ipynb novamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao verificar Spark Session: {e}\")\n",
    "    print(\"\\nüí° SOLU√á√ïES:\")\n",
    "    print(\"1. Execute CONFIGURAR_SPARK.ipynb novamente\")\n",
    "    print(\"2. Verifique processos Java: !ps aux | grep java\")\n",
    "    print(\"3. Reinicie o container: docker restart govbr-jupyter-delta\")\n",
    "    print(\"4. Execute diagn√≥stico: exec(open('/home/jovyan/work/diagnostico_spark.py').read())\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•â Camada Bronze - Dados Brutos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Se os dados Bronze n√£o aparecerem (erro S3A)\n",
    "\n",
    "Execute esta c√©lula para reiniciar o Spark com os JARs corretos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reiniciando Spark com JARs do Hadoop AWS...\n",
      "================================================================================\n",
      "================================================================================\n",
      "üöÄ INICIALIZANDO SPARK COM JARs MANUAIS\n",
      "================================================================================\n",
      "\n",
      "[1/4] Baixando JARs necess√°rios...\n",
      "  ‚úÖ hadoop-aws-3.3.4.jar j√° existe\n",
      "  ‚úÖ aws-java-sdk-bundle-1.12.262.jar j√° existe\n",
      "  ‚úÖ 2 JARs prontos\n",
      "\n",
      "[2/4] Limpando Spark existente...\n",
      "  ‚úÖ Spark limpo\n",
      "\n",
      "[3/4] Matando processos Java...\n",
      "\n",
      "[4/4] Inicializando Spark com JARs...\n",
      "  ‚ÑπÔ∏è  Criando SparkSession com JARs locais...\n",
      "     JARs: /tmp/spark_jars/hadoop-aws-3.3.4.jar,/tmp/spark_jars/aws-java-sdk-bundle-1.12.262.jar\n",
      "  ‚ÑπÔ∏è  Inicializando Spark (aguarde alguns segundos)...\n",
      "  ‚ÑπÔ∏è  Configurando S3A...\n",
      "  ‚ÑπÔ∏è  Verificando funcionalidade...\n",
      "  ‚úÖ Spark inicializado com sucesso!\n",
      "     Vers√£o: 3.5.0\n",
      "     App: GovBR Data Lake - JARs Manuais\n",
      "  ‚ÑπÔ∏è  Testando acesso S3A...\n",
      "  ‚úÖ S3A configurado - tentando ler dados...\n",
      "  ‚úÖ S3A funcionando! Encontrados 5,571 registros em municipios\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SPARK INICIALIZADO COM SUCESSO!\n",
      "================================================================================\n",
      "\n",
      "Agora voc√™ pode ler os dados Bronze:\n",
      "  df = spark.read.parquet('s3a://govbr/bronze/ibge/municipios/')\n",
      "  df.show()\n",
      "\n",
      "‚úÖ Spark reiniciado com sucesso!\n",
      "üí° Execute novamente a c√©lula acima para ver os dados Bronze\n"
     ]
    }
   ],
   "source": [
    "# Reiniciar Spark com JARs do Hadoop AWS\n",
    "# Execute esta c√©lula se houver erro \"ClassNotFoundException: S3AFileSystem\"\n",
    "\n",
    "print(\"üîÑ Reiniciando Spark com JARs do Hadoop AWS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    exec(open('/home/jovyan/work/spark_com_jars_manual.py').read())\n",
    "    print(\"\\n‚úÖ Spark reiniciado com sucesso!\")\n",
    "    print(\"üí° Execute novamente a c√©lula acima para ver os dados Bronze\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao reiniciar Spark: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Tente executar manualmente:\")\n",
    "    print(\"   exec(open('/home/jovyan/work/spark_com_jars_manual.py').read())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Executar Ingest√£o Bronze\n",
    "\n",
    "Se a camada Bronze estiver vazia, execute a c√©lula abaixo para fazer a ingest√£o dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü•â Executando ingest√£o Bronze...\n",
      "================================================================================\n",
      "================================================================================\n",
      "CAMADA BRONZE - INGEST√ÉO DE DADOS BRUTOS\n",
      "================================================================================\n",
      "\n",
      "[1/5] Coletando munic√≠pios do Brasil (IBGE)...\n",
      "‚úÖ Bronze: bronze/ibge/municipios/dt=20251114/data.parquet (5571 registros, 132.35 KB)\n",
      "\n",
      "[2/5] Coletando estados do Brasil (IBGE)...\n",
      "‚úÖ Bronze: bronze/ibge/estados/dt=20251114/data.parquet (27 registros, 4.86 KB)\n",
      "\n",
      "[3/5] Coletando √≥rg√£os SIAFI...\n",
      "‚úÖ Bronze: bronze/portal_transparencia/orgaos_siafi/dt=20251114/data.parquet (6 registros, 2.10 KB)\n",
      "\n",
      "[4/5] Coletando dados de BPC (amostra SP - primeiros 50 munic√≠pios)...\n",
      "  Progresso: 3270/50 munic√≠pios...\n",
      "  Progresso: 3280/50 munic√≠pios...\n",
      "  Progresso: 3290/50 munic√≠pios...\n",
      "  Progresso: 3300/50 munic√≠pios...\n",
      "  Progresso: 3310/50 munic√≠pios...\n",
      "‚úÖ Bronze: bronze/portal_transparencia/bpc_municipios/dt=20251114/data.parquet (50 registros, 10.19 KB)\n",
      "  ‚úÖ 50 munic√≠pios com dados de BPC\n",
      "\n",
      "[4b/5] Coletando dados de Bolsa Fam√≠lia (amostra SP - primeiros 50 munic√≠pios)...\n",
      "  Progresso: 3270/50 munic√≠pios...\n",
      "  Progresso: 3280/50 munic√≠pios...\n",
      "  Progresso: 3290/50 munic√≠pios...\n",
      "  Progresso: 3300/50 munic√≠pios...\n",
      "  Progresso: 3310/50 munic√≠pios...\n",
      "  ‚ö†Ô∏è  Nenhum dado de Bolsa Fam√≠lia coletado (API pode n√£o estar dispon√≠vel)\n",
      "\n",
      "[5/5] Coletando estimativas de popula√ß√£o por estado...\n",
      "  ‚ö†Ô∏è  Endpoint de popula√ß√£o n√£o dispon√≠vel, usando dados manuais: Nenhum dado de popula√ß√£o coletado da API\n",
      "  ‚úÖ 27 estados com dados manuais de popula√ß√£o\n",
      "‚úÖ Bronze: bronze/ibge/populacao_estados/dt=20251114/data.parquet (27 registros, 3.45 KB)\n",
      "\n",
      "================================================================================\n",
      "RESUMO DA INGEST√ÉO\n",
      "================================================================================\n",
      "\n",
      "Total de arquivos na camada Bronze: 5\n",
      "  üìÅ bronze/ibge/estados/dt=20251114/data.parquet (4.86 KB)\n",
      "  üìÅ bronze/ibge/municipios/dt=20251114/data.parquet (132.35 KB)\n",
      "  üìÅ bronze/ibge/populacao_estados/dt=20251114/data.parquet (3.45 KB)\n",
      "  üìÅ bronze/portal_transparencia/bpc_municipios/dt=20251114/data.parquet (10.19 KB)\n",
      "  üìÅ bronze/portal_transparencia/orgaos_siafi/dt=20251114/data.parquet (2.10 KB)\n",
      "\n",
      "Tamanho total: 152.95 KB\n",
      "\n",
      "‚úÖ Ingest√£o Bronze conclu√≠da!\n",
      "\n",
      "‚úÖ Ingest√£o Bronze conclu√≠da!\n",
      "üí° Execute novamente a c√©lula acima para ver os dados Bronze gerados\n"
     ]
    }
   ],
   "source": [
    "# Executar ingest√£o Bronze\n",
    "# ‚ö†Ô∏è Execute esta c√©lula apenas se a camada Bronze estiver vazia\n",
    "\n",
    "print(\"ü•â Executando ingest√£o Bronze...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Executar script de ingest√£o\n",
    "    exec(open('/home/jovyan/work/01_bronze_ingestion.py').read())\n",
    "    \n",
    "    print(\"\\n‚úÖ Ingest√£o Bronze conclu√≠da!\")\n",
    "    print(\"üí° Execute novamente a c√©lula acima para ver os dados Bronze gerados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao executar ingest√£o: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Verifique:\")\n",
    "    print(\"   1. Conex√£o com a internet (APIs externas)\")\n",
    "    print(\"   2. Conex√£o com MinIO\")\n",
    "    print(\"   3. Permiss√µes de escrita no bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Gerar Dados Prata\n",
    "\n",
    "Se a camada Prata n√£o tiver dados, execute a c√©lula abaixo para gerar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Executando transforma√ß√£o Prata...\n",
      "================================================================================\n",
      "================================================================================\n",
      "CAMADA PRATA - TRANSFORMA√á√ÉO E RELACIONAMENTO\n",
      "================================================================================\n",
      "\n",
      "[1/4] Carregando dados da camada Bronze...\n",
      "‚úÖ Lido Bronze: bronze/ibge/municipios/dt=20251114/data.parquet (5571 registros)\n",
      "‚úÖ Lido Bronze: bronze/ibge/estados/dt=20251114/data.parquet (27 registros)\n",
      "‚úÖ Lido Bronze: bronze/portal_transparencia/orgaos_siafi/dt=20251114/data.parquet (6 registros)\n",
      "‚úÖ Lido Bronze: bronze/portal_transparencia/bpc_municipios/dt=20251114/data.parquet (50 registros)\n",
      "‚úÖ Lido Bronze: bronze/ibge/populacao_estados/dt=20251114/data.parquet (27 registros)\n",
      "\n",
      "üìä Status dos dados Bronze carregados:\n",
      "  - Munic√≠pios: ‚úÖ (5571 registros)\n",
      "  - Estados: ‚úÖ (27 registros)\n",
      "  - √ìrg√£os: ‚úÖ (6 registros)\n",
      "  - BPC: ‚úÖ (50 registros)\n",
      "  - Bolsa Fam√≠lia: ‚ùå (0 registros)\n",
      "  - Popula√ß√£o: ‚úÖ (27 registros)\n",
      "\n",
      "[2/4] Tratando e limpando dados...\n",
      "\n",
      "[3/4] Criando dimens√µes e relacionamentos...\n",
      "  ‚úÖ Popula√ß√£o adicionada aos munic√≠pios\n",
      "\n",
      "[Processando BPC] 50 registros encontrados\n",
      "  ‚úÖ Merge com dim_municipios conclu√≠do (50 registros)\n",
      "‚úÖ Prata: prata/fato_bpc/dt=20251114/data.parquet (50 registros, 14.25 KB)\n",
      "  ‚úÖ Fato de BPC salvo com sucesso (50 registros)\n",
      "\n",
      "‚ö†Ô∏è  Dados de Bolsa Fam√≠lia n√£o dispon√≠veis na Bronze\n",
      "   üí° Execute: exec(open('/home/jovyan/work/01_bronze_ingestion.py').read())\n",
      "‚úÖ Prata: prata/dim_municipios/dt=20251114/data.parquet (5571 registros, 140.15 KB)\n",
      "‚úÖ Prata: prata/dim_estados/dt=20251114/data.parquet (27 registros, 9.26 KB)\n",
      "‚úÖ Prata: prata/dim_orgaos/dt=20251114/data.parquet (6 registros, 2.10 KB)\n",
      "\n",
      "[4/4] Criando resumo de transforma√ß√µes...\n",
      "\n",
      "================================================================================\n",
      "RESUMO DA TRANSFORMA√á√ÉO PRATA\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Munic√≠pios tratados: 5571\n",
      "‚úÖ Estados tratados: 27\n",
      "‚úÖ Registros BPC tratados: 50\n",
      "\n",
      "Total de arquivos na camada Prata: 4\n",
      "  üìÅ prata/dim_estados/dt=20251114/data.parquet (9.26 KB)\n",
      "  üìÅ prata/dim_municipios/dt=20251114/data.parquet (140.15 KB)\n",
      "  üìÅ prata/dim_orgaos/dt=20251114/data.parquet (2.10 KB)\n",
      "  üìÅ prata/fato_bpc/dt=20251114/data.parquet (14.25 KB)\n",
      "\n",
      "Tamanho total: 165.76 KB\n",
      "\n",
      "‚úÖ Transforma√ß√£o Prata conclu√≠da!\n",
      "\n",
      "‚úÖ Transforma√ß√£o Prata conclu√≠da!\n",
      "üí° Execute novamente a c√©lula acima para ver os dados Prata gerados\n"
     ]
    }
   ],
   "source": [
    "# Executar transforma√ß√£o Prata\n",
    "# ‚ö†Ô∏è Execute esta c√©lula apenas se a camada Prata estiver vazia\n",
    "\n",
    "print(\"üîÑ Executando transforma√ß√£o Prata...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Verificar se Spark est√° funcional\n",
    "    test_df = spark.range(1)\n",
    "    test_df.collect()\n",
    "    \n",
    "    # Executar script de transforma√ß√£o\n",
    "    exec(open('/home/jovyan/work/02_prata_transformacao.py').read())\n",
    "    \n",
    "    print(\"\\n‚úÖ Transforma√ß√£o Prata conclu√≠da!\")\n",
    "    print(\"üí° Execute novamente a c√©lula acima para ver os dados Prata gerados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao executar transforma√ß√£o: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Verifique se os dados Bronze est√£o dispon√≠veis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü•â CAMADA BRONZE - Dados Brutos\n",
      "\n",
      "‚úÖ municipios                     |      5,571 registros |  11 colunas\n",
      "‚úÖ estados                        |         27 registros |   7 colunas\n",
      "‚úÖ populacao_estados              |         27 registros |   5 colunas\n",
      "‚úÖ orgaos_siafi                   |          6 registros |   3 colunas\n",
      "‚úÖ bpc_municipios                 |         50 registros |  13 colunas\n",
      "\n",
      "üìä Total de datasets Bronze: 5\n"
     ]
    }
   ],
   "source": [
    "# Listar dados Bronze\n",
    "# O Spark l√™ automaticamente parti√ß√µes quando apontamos para o diret√≥rio pai\n",
    "bronze_datasets = [\n",
    "    \"s3a://govbr/bronze/ibge/municipios/\",\n",
    "    \"s3a://govbr/bronze/ibge/estados/\",\n",
    "    \"s3a://govbr/bronze/ibge/populacao_estados/\",\n",
    "    \"s3a://govbr/bronze/portal_transparencia/orgaos_siafi/\",\n",
    "    \"s3a://govbr/bronze/portal_transparencia/bpc_municipios/\"\n",
    "]\n",
    "\n",
    "print(\"ü•â CAMADA BRONZE - Dados Brutos\\n\")\n",
    "bronze_summary = []\n",
    "\n",
    "for path in bronze_datasets:\n",
    "    try:\n",
    "        # Tentar ler - Spark deve detectar parti√ß√µes automaticamente\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        dataset_name = path.split('/')[-2]\n",
    "        bronze_summary.append({\n",
    "            'dataset': dataset_name,\n",
    "            'registros': count,\n",
    "            'colunas': len(df.columns)\n",
    "        })\n",
    "        print(f\"‚úÖ {dataset_name:30s} | {count:>10,} registros | {len(df.columns):>3} colunas\")\n",
    "    except Exception as e:\n",
    "        dataset_name = path.split('/')[-2]\n",
    "        error_msg = str(e)\n",
    "        # Se for erro de classe n√£o encontrada, √© problema de configura√ß√£o S3A\n",
    "        if \"ClassNotFoundException\" in error_msg or \"S3AFileSystem\" in error_msg:\n",
    "            print(f\"‚ö†Ô∏è  {dataset_name:30s} | Erro S3A - Execute: exec(open('/home/jovyan/work/inicializar_spark.py').read())\")\n",
    "        elif \"Path does not exist\" in error_msg or \"does not exist\" in error_msg:\n",
    "            print(f\"‚ö†Ô∏è  {dataset_name:30s} | N√£o dispon√≠vel\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {dataset_name:30s} | Erro: {error_msg[:50]}...\")\n",
    "\n",
    "print(f\"\\nüìä Total de datasets Bronze: {len(bronze_summary)}\")\n",
    "\n",
    "if len(bronze_summary) == 0:\n",
    "    print(\"\\nüí° DICAS:\")\n",
    "    print(\"   1. Verifique se a ingest√£o foi executada: exec(open('/home/jovyan/work/verificar_bronze.py').read())\")\n",
    "    print(\"   2. Se os dados existem mas n√£o aparecem, reinicie o Spark: exec(open('/home/jovyan/work/inicializar_spark.py').read())\")\n",
    "    print(\"   3. Execute a ingest√£o: exec(open('/home/jovyan/work/01_bronze_ingestion.py').read())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Camada Prata - Dados Transformados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Gerar Dados Ouro\n",
    "\n",
    "Se a camada Ouro n√£o tiver dados, execute a c√©lula abaixo para gerar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Executando enriquecimento Ouro...\n",
      "================================================================================\n",
      "‚úÖ Camada Prata dispon√≠vel (5,571 registros em dim_municipios)\n",
      "================================================================================\n",
      "CAMADA OURO - ENRIQUECIMENTO E DADOS FINAIS\n",
      "================================================================================\n",
      "\n",
      "[1/5] Carregando dados da camada Prata...\n",
      "‚úÖ Lido Prata: prata/dim_municipios/dt=20251114/data.parquet (5571 registros)\n",
      "‚úÖ Lido Prata: prata/dim_estados/dt=20251114/data.parquet (27 registros)\n",
      "‚úÖ Lido Prata: prata/fato_bpc/dt=20251114/data.parquet (50 registros)\n",
      "‚úÖ Lido Prata: prata/dim_orgaos/dt=20251114/data.parquet (6 registros)\n",
      "\n",
      "[2/5] Enriquecendo dimens√£o de munic√≠pios...\n",
      "‚úÖ Ouro: ouro/dim_municipios_enriquecida/dt=20251114/data.parquet (5571 registros, 142.47 KB)\n",
      "\n",
      "[3/5] Enriquecendo dimens√£o de estados...\n",
      "‚úÖ Ouro: ouro/dim_estados_enriquecida/dt=20251114/data.parquet (27 registros, 13.78 KB)\n",
      "\n",
      "[4/5] Criando fato BPC enriquecido...\n",
      "‚úÖ Ouro: ouro/fato_bpc_enriquecido/dt=20251114/data.parquet (50 registros, 19.49 KB)\n",
      "\n",
      "[5/5] Criando tabelas agregadas para an√°lise...\n",
      "‚úÖ Ouro: ouro/agregacao_bpc_por_regiao/dt=20251114/data.parquet (1 registros, 6.78 KB)\n",
      "‚úÖ Ouro: ouro/agregacao_bpc_por_estado/dt=20251114/data.parquet (1 registros, 6.62 KB)\n",
      "‚úÖ Ouro: ouro/top_10_municipios_valor_bpc/dt=20251114/data.parquet (10 registros, 4.73 KB)\n",
      "‚úÖ Ouro: ouro/resumo_geral/dt=20251114/data.parquet (4 registros, 2.67 KB)\n",
      "\n",
      "================================================================================\n",
      "RESUMO DO ENRIQUECIMENTO OURO\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Munic√≠pios enriquecidos: 5571\n",
      "‚úÖ Estados enriquecidos: 27\n",
      "‚úÖ Registros BPC enriquecidos: 50\n",
      "\n",
      "Total de arquivos na camada Ouro: 7\n",
      "  üìÅ ouro/agregacao_bpc_por_estado/dt=20251114/data.parquet (6.62 KB)\n",
      "  üìÅ ouro/agregacao_bpc_por_regiao/dt=20251114/data.parquet (6.78 KB)\n",
      "  üìÅ ouro/dim_estados_enriquecida/dt=20251114/data.parquet (13.78 KB)\n",
      "  üìÅ ouro/dim_municipios_enriquecida/dt=20251114/data.parquet (142.47 KB)\n",
      "  üìÅ ouro/fato_bpc_enriquecido/dt=20251114/data.parquet (19.49 KB)\n",
      "  üìÅ ouro/resumo_geral/dt=20251114/data.parquet (2.67 KB)\n",
      "  üìÅ ouro/top_10_municipios_valor_bpc/dt=20251114/data.parquet (4.73 KB)\n",
      "\n",
      "Tamanho total: 196.55 KB\n",
      "\n",
      "‚úÖ Enriquecimento Ouro conclu√≠do!\n",
      "\n",
      "‚úÖ Enriquecimento Ouro conclu√≠do!\n",
      "üí° Execute novamente a c√©lula acima para ver os dados Ouro gerados\n"
     ]
    }
   ],
   "source": [
    "# Executar enriquecimento Ouro\n",
    "# ‚ö†Ô∏è Execute esta c√©lula apenas se a camada Ouro estiver vazia\n",
    "# ‚ö†Ô∏è IMPORTANTE: A camada Prata deve estar gerada primeiro!\n",
    "\n",
    "print(\"üèÜ Executando enriquecimento Ouro...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Verificar se Spark est√° funcional\n",
    "    test_df = spark.range(1)\n",
    "    test_df.collect()\n",
    "    \n",
    "    # Verificar se Prata existe\n",
    "    try:\n",
    "        prata_test = spark.read.parquet(\"s3a://govbr/prata/dim_municipios/\")\n",
    "        prata_count = prata_test.count()\n",
    "        print(f\"‚úÖ Camada Prata dispon√≠vel ({prata_count:,} registros em dim_municipios)\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Camada Prata n√£o est√° dispon√≠vel!\")\n",
    "        print(\"üí° Execute primeiro a transforma√ß√£o Prata (c√©lula acima)\")\n",
    "        raise\n",
    "    \n",
    "    # Executar script de enriquecimento\n",
    "    exec(open('/home/jovyan/work/03_ouro_enriquecimento.py').read())\n",
    "    \n",
    "    print(\"\\n‚úÖ Enriquecimento Ouro conclu√≠do!\")\n",
    "    print(\"üí° Execute novamente a c√©lula acima para ver os dados Ouro gerados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro ao executar enriquecimento: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nüí° Verifique se:\")\n",
    "    print(\"   1. A camada Prata est√° gerada\")\n",
    "    print(\"   2. Os dados Bronze est√£o dispon√≠veis\")\n",
    "    print(\"   3. Spark est√° funcionando corretamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ CAMADA PRATA - Dados Transformados\n",
      "\n",
      "‚úÖ dim_municipios                 |      5,571 registros |  16 colunas\n",
      "‚úÖ dim_estados                    |         27 registros |  13 colunas\n",
      "‚úÖ dim_orgaos                     |          6 registros |   3 colunas\n",
      "‚ö†Ô∏è  fato_bpc                       | N√£o dispon√≠vel\n",
      "‚ö†Ô∏è  fato_bolsa_familia             | N√£o dispon√≠vel\n",
      "\n",
      "üìä Total de datasets Prata: 3\n"
     ]
    }
   ],
   "source": [
    "# Listar dados Prata\n",
    "prata_datasets = [\n",
    "    \"s3a://govbr/prata/dim_municipios/\",\n",
    "    \"s3a://govbr/prata/dim_estados/\",\n",
    "    \"s3a://govbr/prata/dim_orgaos/\",\n",
    "    \"s3a://govbr/prata/fato_bpc/\",\n",
    "    \"s3a://govbr/prata/fato_bolsa_familia/\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ CAMADA PRATA - Dados Transformados\\n\")\n",
    "prata_summary = []\n",
    "\n",
    "for path in prata_datasets:\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        dataset_name = path.split('/')[-2]\n",
    "        prata_summary.append({\n",
    "            'dataset': dataset_name,\n",
    "            'registros': count,\n",
    "            'colunas': len(df.columns)\n",
    "        })\n",
    "        print(f\"‚úÖ {dataset_name:30s} | {count:>10,} registros | {len(df.columns):>3} colunas\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {path.split('/')[-2]:30s} | N√£o dispon√≠vel\")\n",
    "\n",
    "print(f\"\\nüìä Total de datasets Prata: {len(prata_summary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Camada Ouro - Dados Enriquecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ CAMADA OURO - Dados Enriquecidos\n",
      "\n",
      "‚ö†Ô∏è  municipios_enriquecidos        | N√£o dispon√≠vel\n",
      "‚ö†Ô∏è  estados_enriquecidos           | N√£o dispon√≠vel\n",
      "‚ö†Ô∏è  bpc_analytics                  | N√£o dispon√≠vel\n",
      "‚ö†Ô∏è  rankings                       | N√£o dispon√≠vel\n",
      "‚ö†Ô∏è  agregacoes_regionais           | N√£o dispon√≠vel\n",
      "\n",
      "üìä Total de datasets Ouro: 0\n"
     ]
    }
   ],
   "source": [
    "# Listar dados Ouro\n",
    "ouro_datasets = [\n",
    "    \"s3a://govbr/ouro/municipios_enriquecidos/\",\n",
    "    \"s3a://govbr/ouro/estados_enriquecidos/\",\n",
    "    \"s3a://govbr/ouro/bpc_analytics/\",\n",
    "    \"s3a://govbr/ouro/rankings/\",\n",
    "    \"s3a://govbr/ouro/agregacoes_regionais/\"\n",
    "]\n",
    "\n",
    "print(\"üèÜ CAMADA OURO - Dados Enriquecidos\\n\")\n",
    "ouro_summary = []\n",
    "\n",
    "for path in ouro_datasets:\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        dataset_name = path.split('/')[-2]\n",
    "        ouro_summary.append({\n",
    "            'dataset': dataset_name,\n",
    "            'registros': count,\n",
    "            'colunas': len(df.columns)\n",
    "        })\n",
    "        print(f\"‚úÖ {dataset_name:30s} | {count:>10,} registros | {len(df.columns):>3} colunas\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {path.split('/')[-2]:30s} | N√£o dispon√≠vel\")\n",
    "\n",
    "print(f\"\\nüìä Total de datasets Ouro: {len(ouro_summary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà An√°lise Demonstrativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Exemplo de An√°lise: Munic√≠pios por Regi√£o\n",
      "\n",
      "+------------+----------------+\n",
      "|regiao_nome |total_municipios|\n",
      "+------------+----------------+\n",
      "|Nordeste    |1794            |\n",
      "|Sudeste     |1668            |\n",
      "|Sul         |1191            |\n",
      "|Centro-Oeste|467             |\n",
      "|Norte       |450             |\n",
      "|NULL        |1               |\n",
      "+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: An√°lise cruzando as tr√™s camadas\n",
    "try:\n",
    "    # Ler dados de cada camada\n",
    "    df_bronze_municipios = spark.read.parquet(\"s3a://govbr/bronze/ibge/municipios/\")\n",
    "    \n",
    "    print(\"üìä Exemplo de An√°lise: Munic√≠pios por Regi√£o\\n\")\n",
    "    \n",
    "    from pyspark.sql.functions import count, col\n",
    "    \n",
    "    resultado = df_bronze_municipios \\\n",
    "        .groupBy(\"regiao_nome\") \\\n",
    "        .agg(count(\"*\").alias(\"total_municipios\")) \\\n",
    "        .orderBy(col(\"total_municipios\").desc())\n",
    "    \n",
    "    resultado.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro na an√°lise: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Resumo da Arquitetura\n",
    "\n",
    "### Fluxo de Dados:\n",
    "```\n",
    "APIs ‚Üí Bronze ‚Üí Prata ‚Üí Ouro\n",
    "  ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
    "IBGE   Parquet  Dimens√µes  M√©tricas\n",
    "Portal          Fatos      Rankings\n",
    "```\n",
    "\n",
    "### Benef√≠cios:\n",
    "- ‚úÖ **Bronze**: Preserva dados originais\n",
    "- ‚úÖ **Prata**: Dados limpos e relacionados\n",
    "- ‚úÖ **Ouro**: Pronto para an√°lise e dashboards\n",
    "- ‚úÖ **Escal√°vel**: Spark processa grandes volumes\n",
    "- ‚úÖ **Flex√≠vel**: MinIO armazena em formato Parquet\n",
    "\n",
    "### Tecnologias:\n",
    "- Apache Spark 4.0.1\n",
    "- Delta Lake (via JARs)\n",
    "- MinIO (S3-compatible)\n",
    "- Python/PySpark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
