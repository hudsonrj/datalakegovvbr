#!/usr/bin/env python3
"""
Script V2 para corrigir erro Py4J Connection Refused
For√ßa inicializa√ß√£o do gateway Py4J antes de criar SparkConf
"""

import os
import sys
import subprocess
import time
import gc

print("=" * 80)
print("üîß CORRE√á√ÉO Py4J Connection Refused - V2 (Inicializa√ß√£o For√ßada)")
print("=" * 80)

# ============================================================================
# PASSO 1: Limpar completamente ambiente Python/Spark
# ============================================================================
print("\n[1/6] Limpando ambiente completamente...")

# Limpar m√≥dulos Python relacionados ao Spark
modules_to_remove = []
for module_name in list(sys.modules.keys()):
    if 'pyspark' in module_name.lower() or 'spark' in module_name.lower() or 'py4j' in module_name.lower():
        modules_to_remove.append(module_name)

for module_name in modules_to_remove:
    try:
        del sys.modules[module_name]
    except:
        pass

print(f"  ‚úÖ {len(modules_to_remove)} m√≥dulos removidos")

# Limpar vari√°veis globais
try:
    import __main__
    for var_name in ['spark', 'sc', 'sparkContext', 'SparkSession', 'SparkContext']:
        if hasattr(__main__, var_name):
            delattr(__main__, var_name)
except:
    pass

# Garbage collection m√∫ltiplo
for _ in range(5):
    gc.collect()
    time.sleep(0.3)

# Matar processos Java
print("  ‚ÑπÔ∏è  Matando processos Java...")
for pattern in ["org.apache.spark", "java.*spark", "SparkSubmit"]:
    try:
        result = subprocess.run(
            ["pgrep", "-f", pattern],
            capture_output=True,
            text=True,
            timeout=3
        )
        if result.returncode == 0:
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                if pid and pid.isdigit():
                    try:
                        subprocess.run(["kill", "-9", pid], check=False, timeout=1)
                    except:
                        pass
    except:
        pass

time.sleep(2)

# ============================================================================
# PASSO 2: Configurar JAVA_HOME corretamente
# ============================================================================
print("\n[2/6] Configurando JAVA_HOME...")

java_home_candidates = [
    '/usr/lib/jvm/java-17-openjdk-amd64',
    '/usr/lib/jvm/java-11-openjdk-amd64',
    '/usr/lib/jvm/java-8-openjdk-amd64',
]

java_home = None
for candidate in java_home_candidates:
    if os.path.exists(candidate) and os.path.exists(f"{candidate}/bin/java"):
        java_home = candidate
        break

if not java_home:
    try:
        result = subprocess.run(["which", "java"], capture_output=True, text=True, timeout=2)
        if result.returncode == 0:
            java_path = result.stdout.strip()
            # Tentar encontrar JAVA_HOME
            if '/usr/lib/jvm' in java_path:
                parts = java_path.split('/')
                for i, part in enumerate(parts):
                    if part == 'jvm' and i + 1 < len(parts):
                        java_home = '/'.join(parts[:i+2])
                        break
    except:
        pass

if java_home:
    os.environ['JAVA_HOME'] = java_home
    print(f"  ‚úÖ JAVA_HOME: {java_home}")
    
    # Verificar Java
    try:
        result = subprocess.run(
            [f"{java_home}/bin/java", "-version"],
            capture_output=True,
            text=True,
            timeout=2
        )
        if result.returncode == 0 or result.stderr:
            version_line = result.stderr.split('\n')[0] if result.stderr else "OK"
            print(f"  ‚úÖ Java verificado: {version_line[:50]}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Aviso ao verificar Java: {e}")
else:
    print("  ‚ö†Ô∏è  JAVA_HOME n√£o detectado automaticamente")

# Configurar vari√°veis de ambiente
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'
os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'
os.environ['SPARK_DRIVER_HOST'] = '127.0.0.1'
os.environ['SPARK_DRIVER_BIND_ADDRESS'] = '127.0.0.1'

# ============================================================================
# PASSO 3: Importar PySpark e limpar refer√™ncias internas
# ============================================================================
print("\n[3/6] Importando PySpark e limpando refer√™ncias internas...")

try:
    from pyspark import SparkContext
    
    # Limpar TODAS as refer√™ncias internas do SparkContext
    if hasattr(SparkContext, '_active_spark_context'):
        SparkContext._active_spark_context = None
    if hasattr(SparkContext, '_jvm'):
        SparkContext._jvm = None
    if hasattr(SparkContext, '_gateway'):
        SparkContext._gateway = None
    if hasattr(SparkContext, '_next_accumulator_id'):
        SparkContext._next_accumulator_id = 0
    
    print("  ‚úÖ Refer√™ncias SparkContext limpas")
    
    from pyspark.sql import SparkSession
    
    # Limpar refer√™ncias SparkSession
    if hasattr(SparkSession, '_instantiatedContext'):
        SparkSession._instantiatedContext = None
    if hasattr(SparkSession, '_instantiatedSession'):
        SparkSession._instantiatedSession = None
    
    print("  ‚úÖ Refer√™ncias SparkSession limpas")
    
except ImportError as e:
    print(f"  ‚ùå Erro ao importar PySpark: {e}")
    raise RuntimeError(f"PySpark n√£o est√° dispon√≠vel: {e}")

# ============================================================================
# PASSO 4: For√ßar inicializa√ß√£o do gateway Py4J ANTES de criar SparkConf
# ============================================================================
print("\n[4/6] Inicializando gateway Py4J manualmente...")

try:
    from py4j.java_gateway import JavaGateway, GatewayParameters
    
    # Tentar criar gateway Py4J manualmente
    # Isso for√ßa a inicializa√ß√£o do JVM antes do SparkConf
    print("  ‚ÑπÔ∏è  Criando gateway Py4J...")
    
    # Configurar par√¢metros do gateway
    gateway_params = GatewayParameters(
        auto_convert=True,
        auto_field=True,
        auto_close=True
    )
    
    # Criar gateway - isso vai iniciar o JVM
    gateway = JavaGateway(gateway_parameters=gateway_params)
    
    # Verificar se gateway est√° funcionando
    try:
        jvm = gateway.jvm
        print("  ‚úÖ Gateway Py4J criado e JVM inicializado")
        
        # Testar comunica√ß√£o b√°sica
        try:
            # Tentar criar um objeto Java simples
            test_obj = jvm.java.lang.String("test")
            test_str = str(test_obj)
            print(f"  ‚úÖ Comunica√ß√£o Py4J testada: {test_str}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Aviso no teste Py4J: {e}")
        
        # Fechar gateway manual - vamos deixar o Spark criar o seu pr√≥prio
        gateway.shutdown()
        time.sleep(1)
        
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Erro ao inicializar gateway: {e}")
        # Tentar fechar se existir
        try:
            gateway.shutdown()
        except:
            pass
        raise
    
except ImportError:
    print("  ‚ö†Ô∏è  py4j n√£o dispon√≠vel diretamente, Spark vai inicializar automaticamente")
except Exception as e:
    print(f"  ‚ö†Ô∏è  Erro ao criar gateway manual: {e}")
    print("  ‚ÑπÔ∏è  Continuando - Spark vai tentar inicializar automaticamente")

# ============================================================================
# PASSO 5: Criar SparkConf e SparkSession
# ============================================================================
print("\n[5/6] Criando SparkConf e SparkSession...")

try:
    from pyspark.sql import SparkSession
    from pyspark import SparkConf
    
    # Configura√ß√µes MinIO
    MINIO_ENDPOINT = "ch8ai-minio.l6zv5a.easypanel.host"
    MINIO_ACCESS_KEY = "admin"
    MINIO_SECRET_KEY = "1q2w3e4r"
    
    print("  ‚ÑπÔ∏è  Criando SparkConf...")
    
    # Criar SparkConf - agora o JVM j√° deve estar inicializado
    conf = SparkConf()
    conf.set("spark.app.name", "GovBR Data Lake - Fixed V2")
    conf.set("spark.master", "local[*]")
    conf.set("spark.driver.host", "127.0.0.1")
    conf.set("spark.driver.bindAddress", "127.0.0.1")
    conf.set("spark.driver.port", "0")
    conf.set("spark.blockManager.port", "0")
    conf.set("spark.broadcast.port", "0")
    conf.set("spark.fileserver.port", "0")
    conf.set("spark.replClassServer.port", "0")
    conf.set("spark.ui.port", "0")
    conf.set("spark.driver.memory", "2g")
    conf.set("spark.executor.memory", "2g")
    conf.set("spark.network.timeout", "1200s")
    conf.set("spark.executor.heartbeatInterval", "60s")
    conf.set("spark.driver.extraJavaOptions", 
            "-Dio.netty.tryReflectionSetAccessible=true " +
            "-XX:+UseG1GC " +
            "-Djava.net.preferIPv4Stack=true " +
            "-Djava.awt.headless=true")
    
    # Configura√ß√µes S3A
    conf.set("spark.hadoop.fs.s3a.endpoint", f"https://{MINIO_ENDPOINT}")
    conf.set("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY)
    conf.set("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY)
    conf.set("spark.hadoop.fs.s3a.path.style.access", "true")
    conf.set("spark.hadoop.fs.s3a.connection.ssl.enabled", "true")
    conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    
    print("  ‚úÖ SparkConf criado")
    
    # Criar SparkSession usando o conf
    print("  ‚ÑπÔ∏è  Criando SparkSession (isso pode levar alguns segundos)...")
    
    spark = SparkSession.builder.config(conf=conf).getOrCreate()
    
    # Aguardar inicializa√ß√£o
    time.sleep(5)
    
    # Verificar funcionalidade
    print("  ‚ÑπÔ∏è  Verificando funcionalidade...")
    version = spark.version
    app_name = spark.sparkContext.appName
    
    print(f"  ‚úÖ Spark Session criada!")
    print(f"     Vers√£o: {version}")
    print(f"     App: {app_name}")
    
    # Teste funcional
    test_df = spark.range(5)
    count = test_df.count()
    
    if count == 5:
        print(f"  ‚úÖ Teste funcional: {count} registros criados")
    else:
        raise RuntimeError(f"Teste retornou resultado inesperado: {count}")
    
except Exception as e:
    print(f"  ‚ùå Erro ao criar Spark Session: {e}")
    import traceback
    traceback.print_exc()
    raise RuntimeError(f"Falha ao criar Spark Session: {e}")

# ============================================================================
# PASSO 6: Tornar dispon√≠vel globalmente
# ============================================================================
print("\n[6/6] Finalizando...")

try:
    import __main__
    __main__.spark = spark
    globals()['spark'] = spark
    
    print("  ‚úÖ Spark Session dispon√≠vel como vari√°vel global 'spark'")
    
    print("\n" + "=" * 80)
    print("‚úÖ CORRE√á√ÉO CONCLU√çDA COM SUCESSO!")
    print("=" * 80)
    print("\nA Spark Session est√° pronta para uso.")
    print("Use 'spark' para acessar a sess√£o nos seus notebooks.")
    
except Exception as e:
    print(f"  ‚ö†Ô∏è  Erro ao finalizar: {e}")
