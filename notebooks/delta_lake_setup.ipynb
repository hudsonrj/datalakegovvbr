{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake Setup - Configuração Completa\n",
        "\n",
        "Este notebook configura o Delta Lake com suporte completo ao S3A (MinIO).\n",
        "\n",
        "## ⚠️ IMPORTANTE\n",
        "\n",
        "**Execute TODAS as células em ordem!**\n",
        "\n",
        "1. Primeira célula: Configura PYSPARK_SUBMIT_ARGS ANTES de importar Spark\n",
        "2. Segunda célula: Para qualquer Spark Session existente\n",
        "3. Terceira célula: Cria Spark Session com JARs corretos\n",
        "4. Quarta célula: Executa setup do Delta Lake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CÉLULA 1: Limpar PYSPARK_SUBMIT_ARGS para evitar conflitos\n",
        "# IMPORTANTE: Não configurar PYSPARK_SUBMIT_ARGS aqui - vamos usar apenas spark.jars.packages\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Configurar paths\n",
        "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3'\n",
        "\n",
        "# Limpar PYSPARK_SUBMIT_ARGS se existir para evitar conflitos\n",
        "if 'PYSPARK_SUBMIT_ARGS' in os.environ:\n",
        "    print(\"⚠️  Removendo PYSPARK_SUBMIT_ARGS existente para evitar conflitos\")\n",
        "    del os.environ['PYSPARK_SUBMIT_ARGS']\n",
        "\n",
        "print(\"✅ Ambiente configurado para usar apenas spark.jars.packages\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CÉLULA 2: Configurar Java e parar qualquer Spark Session existente\n",
        "import os\n",
        "\n",
        "# Configurar JAVA_HOME explicitamente\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
        "print(f\"✅ JAVA_HOME configurado: {os.environ['JAVA_HOME']}\")\n",
        "\n",
        "# Parar qualquer Spark Session existente\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    existing_spark = SparkSession.getActiveSession()\n",
        "    if existing_spark:\n",
        "        print(\"⚠️  Parando Spark Session existente...\")\n",
        "        existing_spark.stop()\n",
        "        import time\n",
        "        time.sleep(5)  # Aguardar encerramento completo\n",
        "        print(\"✅ Spark Session parada\")\n",
        "    else:\n",
        "        print(\"✅ Nenhuma Spark Session ativa\")\n",
        "except Exception as e:\n",
        "    print(f\"ℹ️  {e}\")\n",
        "    print(\"✅ Pronto para criar nova Spark Session\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CÉLULA 3: Criar Spark Session com JARs do S3A e Delta Lake\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "# Configurações MinIO\n",
        "MINIO_ENDPOINT = \"ch8ai-minio.l6zv5a.easypanel.host\"\n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"1q2w3e4r\"\n",
        "BUCKET_NAME = \"govbr\"\n",
        "\n",
        "print(\"\\n[INFO] Criando Spark Session com JARs do S3A e Delta Lake...\")\n",
        "print(\"[INFO] Isso pode demorar 2-5 minutos na primeira execução enquanto baixa os JARs...\")\n",
        "\n",
        "# JARs do S3A e Delta Lake\n",
        "# IMPORTANTE: Usar versão compatível com Spark 4.0.1\n",
        "s3a_packages = \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
        "delta_package = \"io.delta:delta-spark_2.13:4.0.0\"\n",
        "\n",
        "try:\n",
        "    builder = SparkSession.builder \\\n",
        "        .appName(\"GovBR Delta Lake\") \\\n",
        "        .config(\"spark.jars.packages\", f\"{delta_package},{s3a_packages}\") \\\n",
        "        .config(\"spark.driver.memory\", \"2g\") \\\n",
        "        .config(\"spark.executor.memory\", \"2g\") \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"https://{MINIO_ENDPOINT}\") \\\n",
        "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
        "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
        "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
        "        .config(\"spark.master\", \"local[*]\") \\\n",
        "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
        "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\n",
        "    \n",
        "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "    \n",
        "    print(\"\\n✅ Spark Session criada!\")\n",
        "    print(f\"✅ Versão Spark: {spark.version}\")\n",
        "    \n",
        "    # Verificar se os JARs estão no classpath\n",
        "    try:\n",
        "        spark_context = spark.sparkContext\n",
        "        packages = spark_context.getConf().get(\"spark.jars.packages\", \"\")\n",
        "        if \"hadoop-aws\" in packages:\n",
        "            print(\"\\n✅ JARs do S3A encontrados no classpath!\")\n",
        "            print(f\"   Packages: {packages[:200]}...\")\n",
        "        else:\n",
        "            print(\"\\n⚠️  JARs do S3A podem não estar no classpath\")\n",
        "            print(f\"   Packages: {packages}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠️  Erro ao verificar classpath: {e}\")\n",
        "    \n",
        "    import time\n",
        "    print(\"\\n[INFO] Aguardando carregamento completo dos JARs...\")\n",
        "    time.sleep(3)\n",
        "    print(\"✅ Pronto para usar Delta Lake!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Erro ao criar Spark Session: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CÉLULA 4: Executar setup do Delta Lake\n",
        "# Este script converte os dados Parquet em Delta Lake\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXECUTANDO SETUP DO DELTA LAKE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Executar o script de setup\n",
        "exec(open('delta_setup_spark.py').read())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}