{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagn√≥stico Delta Lake\n",
    "\n",
    "Este notebook verifica:\n",
    "1. Se os arquivos Parquet existem\n",
    "2. Se o Spark consegue ler os arquivos\n",
    "3. Se as tabelas Delta foram criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Spark Session (reutilizar da c√©lula anterior do delta_lake_setup.ipynb)\n",
    "# Se n√£o tiver executado, execute primeiro as c√©lulas 1-3 do delta_lake_setup.ipynb\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Configura√ß√µes MinIO\n",
    "MINIO_ENDPOINT = \"ch8ai-minio.l6zv5a.easypanel.host\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"1q2w3e4r\"\n",
    "BUCKET_NAME = \"govbr\"\n",
    "\n",
    "s3a_packages = \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "delta_package = \"io.delta:delta-spark_2.13:4.0.0\"\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Diagn√≥stico Delta\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{delta_package},{s3a_packages}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"https://{MINIO_ENDPOINT}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "print(\"‚úÖ Spark Session criada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se os arquivos Parquet existem\n",
    "s3_base = f\"s3a://{BUCKET_NAME}\"\n",
    "\n",
    "parquet_paths = {\n",
    "    \"bronze_municipios\": f\"{s3_base}/bronze/ibge/municipios/dt=20251114/data.parquet\",\n",
    "    \"bronze_estados\": f\"{s3_base}/bronze/ibge/estados/dt=20251114/data.parquet\",\n",
    "    \"bronze_bpc\": f\"{s3_base}/bronze/portal_transparencia/bpc_municipios/dt=20251114/data.parquet\",\n",
    "    \"prata_dim_municipios\": f\"{s3_base}/prata/dim_municipios/dt=20251114/data.parquet\",\n",
    "    \"prata_dim_estados\": f\"{s3_base}/prata/dim_estados/dt=20251114/data.parquet\",\n",
    "    \"prata_fato_bpc\": f\"{s3_base}/prata/fato_bpc/dt=20251114/data.parquet\",\n",
    "    \"ouro_dim_municipios\": f\"{s3_base}/ouro/dim_municipios_enriquecida/dt=20251114/data.parquet\",\n",
    "    \"ouro_dim_estados\": f\"{s3_base}/ouro/dim_estados_enriquecida/dt=20251114/data.parquet\",\n",
    "    \"ouro_fato_bpc\": f\"{s3_base}/ouro/fato_bpc_enriquecido/dt=20251114/data.parquet\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICANDO ARQUIVOS PARQUET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, path in parquet_paths.items():\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        print(f\"‚úÖ {name}: {count} registros - {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name}: ERRO - {str(e)[:100]}\")\n",
    "        print(f\"   Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tabelas Delta existentes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABELAS DELTA EXISTENTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    if tables:\n",
    "        for table in tables:\n",
    "            print(f\"üìä {table.tableName}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Nenhuma tabela encontrada\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao listar tabelas: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentar criar uma tabela Delta de teste\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTE: CRIAR TABELA DELTA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_parquet = f\"{s3_base}/bronze/ibge/estados/dt=20251114/data.parquet\"\n",
    "test_delta = f\"{s3_base}/delta/test/test_estados\"\n",
    "test_table = \"test_estados\"\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1. Lendo Parquet: {test_parquet}\")\n",
    "    df = spark.read.parquet(test_parquet)\n",
    "    count = df.count()\n",
    "    print(f\"   ‚úÖ Lido: {count} registros\")\n",
    "    \n",
    "    print(f\"\\n2. Escrevendo Delta: {test_delta}\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(test_delta)\n",
    "    print(f\"   ‚úÖ Delta escrito\")\n",
    "    \n",
    "    print(f\"\\n3. Criando tabela: {test_table}\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {test_table}\n",
    "        USING DELTA\n",
    "        LOCATION '{test_delta}'\n",
    "    \"\"\")\n",
    "    print(f\"   ‚úÖ Tabela criada\")\n",
    "    \n",
    "    print(f\"\\n4. Verificando tabela:\")\n",
    "    result = spark.sql(f\"SELECT COUNT(*) as cnt FROM {test_table}\").collect()\n",
    "    print(f\"   ‚úÖ Tabela funciona: {result[0]['cnt']} registros\")\n",
    "    \n",
    "    print(f\"\\n5. Mostrando dados:\")\n",
    "    spark.sql(f\"SELECT * FROM {test_table} LIMIT 3\").show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
